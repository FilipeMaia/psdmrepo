:
:  Library of Jinja2 templates for Hdf5Translator backend
:
:  Lines starting with colon are comments, except for special '::::template::::'
:
::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
::::template:::: macros
::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
::::template:::: psana_cfg_template
::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
:
:  Template for a psana.cfg file that uses the Hdf5 Translator module.
:  This sets default values for all of the type filters.
:  
:  Parameters for this template:
:
: type_filter_options - a multiline list of the aliases, includes, and a comment
:                       to list all the types associated with the alias
######################################################################
[psana]

# MODULES: any modules that produce data to be translated need be loaded 
# **BEFORE** Translator.H5Output (such as calibrated data or ndarray's)
# event data added by modules listed after Translator.H5Output is not translated.
modules = Translator.H5Output

files = **TODO: SPECIFY INPUT FILES OR DATA SOURCE HERE**

######################################################################
[Translator.H5Output]

# The only option you need to set for the Translator.H5Output module is
# output_file. All other options have default values (explained below).

# TODO: enter the full h5 output file name, including the output directory
output_file = output_directory/h5output.h5

# By default, the Translator will not overwrite the h5 file if it already exists
overwrite = false

# # # # # # # # # # # # # # # # # # # # #
# EPICS FILTERING
# The Translator can store epics pv's in one of three ways, or not at all.
# set store_epics below, to one of the following:
#
# updates_only   stores an EPICS pv when it has been updated in the psana epics store.
#                For xtc input this happens whenever EPICS data is present in a datagram.
#                Note - many EPICS pvs are not present in every shot. A dataset
#                for an EPIC pv is often shorter than the total number of events.
#                Experiments with many short calib cycles may have some calib cycles where
#                no EPICS pv's show up. Users would then have to look back through several 
#                calib cycle's to find the latest value of a pv.
#
# calib_repeat   This is the same as updates_only except that each calib cycle starts with
#                the most recent value of each pv. This makes it easier to find pv's in a 
#                calib cycle. For experiments with many short calib cycles, it can produce
#                more datasets than neccessary.
#
# always         For each event, store the most recent value of the EPICs pv. Produces 
#                longer datasets than neccessary, but makes it easier to find the latest
#                pv for an event.
#
# no             epics pv's will not be stored. You may also want to set Epics=exclude
#                (see below) if you do not want the epics configuration data stored

# The default is 'calib_repeat'

store_epics = calib_repeat

# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
# FILTERING
# 
# By default, all xtc data is Translated and many ndarrays that user modules (if any) 
# add are translated. Filtering can occur in either the code of user modules, or
# through options in the psana.cfg file. Here in the config file, different groups of 
# data can be filtered. There are four options for filtering data: 
#
#    type filtering   -  for example, exclude all cspad, regardless of the detector source
#    source filtering -  for example, exclude any data from a given detector source
#    key filtering    -  for example, include only ndarrays with a given key string
#    calibration      -  do not translate original xtc if a calibrated version is found
#
# Type filtering is based on sets of Psana data types. If you know what detectors or 
# devices to filter, leave type filtering alone and go to src_filter. 
#
# Type filtering has the highest precedence, then key filtering, then source 
# filtering, and lastly calibration filtering. When the Translator sees new data, 
# it first checks the type filter. If it is a filtered type (or unknown type) no further 
# translation occurs with the data - regardless of src or key. For data that gets 
# past the type filter, the Translator looks at the src and key. If the key 
# string is empty, it checks the source filter. Data with non empty key strings are 
# handled via the key filter. If the src is filtered, but the key is not, then the
# data will be translated. Data with the special calibration key string are handled 
# via the calibration filtering. 
#
# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
# TYPE FILTERING 
#
# One can include or exclude a class of Psana types with the following 
# options. Only the strings include or exclude are valid for these 
# type filtering options. 
# 
# Note - Epics in the list below refers only to the epicsConfig data
# which is the epics alias list, not the epics pv's. To filter the epics pv's
# see the 'store_epics' option above.

{{type_filter_options}}

# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
# TYPE FILTER SHORTCUT
#
# In addition to filtering Psana types by the options above, one can use
# the type_filter option below. For example:
#
# type_filter = include cspad       # will only translate cspad types. Will not translate
#                                 # ndarrays or strings
# type_filter = exclude Andor evr   # translate all except the Andor or Evr types
# 
# If you do not want to translate what is in the xtc file, use the psana shortcut:
#
# type_filter = exclude psana       # This will only translate ndarray's and strings 
#
# Likewise doing:
#
# type_filter = include psana       # will translate all xtc data, but skip any ndarray's or strings
#
# The default is to include all

type_filter = include all

# note - if type_filter is anything other than 'include all' it takes precedence
# over the classes of type filter options above, like Cspad=include.

# # # # # # # # # # # # # # # # # # # # # # # # # # # # #
# SOURCE FILTERING
#
# The default for the src_filter option is "include all"
# If you want to include a subset of the sources, do
#
# src_filter include srcname1 srcname2  
#
#  or if you want to exclude a subset of sources, do
#
# src_filter exclude srcname1 srcname2
#
# The syntax for specifying a srcname follows that of the Psana Source (discussed in 
# the Psana Users Guide). The Psana Source recognizes DAQ alias names (if present
# in the xtc files), several styles for specifying a Pds Src, as well as detector matches 
# where the detector number, or device number is not known.
# 
# Specifically, format of the match string can be:
#
#       DetInfo(det.detId:dev.devId) - fully or partially specified DetInfo
#       det.detId:dev.devId - same as above
#       DetInfo(det-detId|dev.devId) - same as above
#       det-detId|dev.devId - same as above
#       BldInfo(type) - fully or partially specified BldInfo
#       type - same as above
#       ProcInfo(ipAddr) - fully or partially specified ProcInfo
#
# For example
#        DetInfo(AmoETOF.0.Acqiris.0)  
#        DetInfo(AmoETOF.0.Acqiris)  
#        DetInfo(AmoETOF:Acqiris)
#        AmoETOF:Acqiris
#        AmoETOF|Acqiris
#
# will all match the same data, AmoETOF.0.Acqiris.0. The later ones will match
# additional data (such as detector 1, 2, etc.) if it is present.
#
# A simple way to set up src filtering is to take a look at the sources in the 
# xtc input using the psana EventKeys module.  For example
#
# psana -n 5 -m EventKeys exp=cxitut13:run=22 
#
# Will print the EventKeys in the first 5 events.  If the output includes
#
#   EventKey(type=Psana::EvrData::DataV3, src=DetInfo(NoDetector.0:Evr.2))
#   EventKey(type=Psana::CsPad::DataV2, src=DetInfo(CxiDs1.0:Cspad.0))
#   EventKey(type=Psana::CsPad2x2::ElementV1, src=DetInfo(CxiSc2.0:Cspad2x2.1))
#   EventKey(type=Psana::Bld::BldDataEBeamV3, src=BldInfo(EBeam))
#   EventKey(type=Psana::Bld::BldDataFEEGasDetEnergy, src=BldInfo(FEEGasDetEnergy))
#   EventKey(type=Psana::Camera::FrameV1, src=BldInfo(CxiDg2_Pim))
#
# Then one can filter on these six srcname's:
#
#  NoDetector.0:Evr.2  CxiDs1.0:Cspad.0  CxiSc2.0:Cspad2x2.1  EBeam  FEEGasDetEnergy  CxiDg2_Pim
#

src_filter = include all

# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
# CALIBRATION FILTERING
#
# Psana calibration modules can produce calibrated versions of different 
# data types. Depending on the module used, you may get an NDArray, an 
# image, or the same data type as was in the xtc but with calibrated data.
#
# If you are doing the latter, the module output will be data of the same type 
# and src as the uncalibrated data, with an additional key, such as 'calibrated'.
# If these modules are configured to use a different key, set calibration_key
# below accordingly:

calibration_key = calibrated

# The Translator defaults to writing calibrated data in place of uncalibrated
# data. If you do not want the calibrated data and would prefer to have the
# original uncalibrated data from the xtc, then set skip_calibrated to true.

skip_calibrated = false

# note, setting skip_calibrated to true will force sets exclude_calibstore 
# (below) to be true as well.

# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
# CALIBSTORE FILTERING
#
# Calibration modules may publish the data they used to produce the calibrated
# event objects. Examples of data would be pedestal values, pixel status (what
# pixels are hot) and common mode algorithm parameters. This data will be published
# in what is called the Psana calibStore. When the Translator sees calibrated 
# event data, it will look for the corresponsinding calibStore data as well.
# If you do not want it to translate calibStore data, set the following to true.

exclude_calibstore = false

# otherwise, the Translator will create a group CalibStore that holds the
# calibstore data. Note, the Translator looks for all calibStore data associated 
# with the calibration modules. If a calibration module was configured to not do 
# certain calibrations (such as gain) but the module still put gain values
# in the config store (even though it did not use them) the Translator 
# would still translate those gain values.

# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
# KEY FILTERING
#
# Psana modules loaded before the translator may put a variety of objects in the event 
# store. Be default, the Translator will translate any new data that it knows about.
# In addition to the psana types, it knows about NDArrays, C++ strings, and has a C++ interface 
# for registering new simple types. NDarray's up to 4 dimensions of 10 basic types 
# (8, 16, 32 and 64 bit signed and unsigned int, float and double) as well as the const 
# versions of these types are translated.
#
# Generally Psana modules will attach keys to these objects (the keys are simply strings).
# To filter the set of keys that are translated, modify the parameter below:

key_filter = include all

# The default is to not look at the key but rather translate all data that the translator
# knows about. An example of including only data with the key finalanswer would be
#
# key_filter = include finalanswer
#
# To exclude a few keys, one can do
#
# key_filter = exclude arrayA arrayB
#
# Note, key filtering does not affect translation of data without keys. For instance
# setting key_filter = include keyA does not turn off translation of data without keys.
# Of all the data with keys, only those where the key is keyA will be translated.
#

# ---------------------------------------
# SPLIT INTO SEPARTE HDF5 FILES BASED ON CALIB CYCLES
#
# There are two reasons to split the Translator output into separate files based on
# calib cycles. The first is to reduce the size of the hdf5 files, and the second is
# to speedup translation by translating separate calib cycles in parallel. The default 
# is to not split:

split = NoSplit

# however the Translator also supports SplitScan mode. The best way to invoke this is to
# run the separate driver program
#
#  h5-mpi-translate
#
# which requires MPI to be available in the environment, but see below for the non-MPI 
# based split scan mode. In SplitScan mode, in addition to the output File, separate files will 
# be made for the calib cycles. The output file (the master file) will include external links 
# to the other files. Several mpi jobs are run simultaneously to divide the work of creating the 
# calib cycle files. For example, running six jobs to produce out.h5 might look like:
#
#   mpirun -n 6 h5-mpi-translate -m Translator.H5Output -o Translator.H5Output.output_file=out.h5 exp=xppd9714:run=16
#
# The driver program, h5-mpi-translate, takes all arguments that psana takes.
# Presently with our implementation, one must give the full path to mpirun, and run
# a driver program that sets up the analysis environment. For example, 
#
#   /reg/g/psdm/sw/releases/ana-current/arch/x86_64-rhel6-gcc44-opt/bin/mpirun -n 6 mpilaunch h5-mpi-translate -m Translator.H5Output -o Translator.H5Output.output_file=out.h5 exp=xppd9714:run=16
#
# where mpilaunch is
#
#  #!/bin/bash
#  . /reg/g/psdm/bin/sit_setup.sh ana-current
#  $@
#
# If six jobs were used, one becomes the master process and the other five are the workers.
# The master process does two things. First it writes the file out.h5 with the external links 
# to the calib files. Second it reads through all the data and finds the calib cycles. When it
# finds a calib cycle, it tells the next available worker where this is. When a worker is done,
# it tells the master process. The master process than adds all neccessary external links from
# out.h5 to the translated calib file produced by the worker.
#
# Generally, there will be one calib cycle file for each calib cycle. However to prevent to many 
# calib cycle files from being produced for experiments that have only a few events per calib cycle, 
# an option controls the minumum number of events per external calib cycle file. The default is

# min_events_per_calib_file = 100

# For example, if there are only 10 events per calib cycle, and assuming the master file is called 
# out.h5, the file output_cc0000.h5 will contain the groups 
#
# /CalibCycle:0000
# /CalibCycle:0001
# ...
# /CalibCycle:0009
#
# and the file output_cc0010.h5 will start with group /CalibCycle:0010
#
# As mentioned above, when workers finish a calib cycle file, they send a message to the master. 
# How frequently the master stops reading through the data to check for these messages is controlled 
# by the following parameter

# num_events_check_done_calib_file = 120

# that is, it defaults to check for a 'done' message from a worker every 120 events.
#
# When running the h5-mpi-translate and specifying user psana modules (perhaps to add ndarrays
# into the translation or dynamically filter events) it is important to note that these modules
# are restarted for each calib cycle file. That is these modules will have their beginJob/endJob
# and beginRun/endRun routines called for each calib file that a worker produces.
#
# If MPI is not available, the Translator supports an additional  split scan mode by setting the 
# split option as follows:
#
# split=SplitScan
#
# In this mode, there is no communication between jobs, and each Translator job reads through all 
# the input, so launching too many jobs will significantly increase the amount of input processing. 
# Dividing the work of this SplitScan mode is done with the parameters

# jobTotal = 1
# jobNumber = 0

# which default to 1 job that is numbered 0. However if jobTotal=3 and jobNumber=1, this 
# Translator will process calibCycle 1, 4, 7, etc. If jobTotal is 3, the user MUST
# make sure to launch 3 Translator jobs with jobNumber being 0,1 and 2 to get all the calib cycle
# files written. jobNumber=0 will write the master file with the external links to the calib
# cycle files. 
#
# For example, the following two command lines:
#
# psana -m Translator.H5Output -o Translator.H5Output.output_file=mydir/split.h5 -o Translator.H5Output.split=SplitScan -o Translator.H5Output.jobNumber=0 -o Translator.H5Output.jobTotal=2 exp=xpp123:run=10
# psana -m Translator.H5Output -o Translator.H5Output.output_file=mydir/split.h5 -o Translator.H5Output.split=SplitScan -o Translator.H5Output.jobNumber=1 -o Translator.H5Output.jobTotal=2 exp=xpp123:run=10
#
# will divide the work into two translator jobs. When they finish, the output will be
# 
# mydir/split.h5
# mydir/split_cc0000.h5
# mydir/split_cc0001.h5
# ...
#
# note, this split scan mode, unlike the MPI version, does not put multiple calib cycles in
# one file. One can get a large number of files if calib cycles only contain a few events.

# The remaining values for split are
#
# split=MPIWorker
# split=MPIMaster
#
# These values are meant to be set by h5-mpi-translate. Generally users should not set
# these values. The h5-mpi-translate driver will set additional options that we 
# document here (again, users should not need to set these options). These options include the
# two discussed above:
#
#   num_events_check_done_calib_file 
#   min_events_per_calib_file
#
# as well as the option
#
# first_calib_cycle_number
#
# which is a 0-up counter for the first calib cycle that the MPIWorker will see.

# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # 
# COMPRESSION 
#
# The following options control compression for most all datasets.
# Shuffling improves compression for certain datasets. Valid values for
# deflate (gzip compression level) are 0-9. Setting deflate = -1 turns off
# compression.

shuffle = true
deflate = 1

# if deflate is set to -1, set shuffle to false, as it performs no function without compression.

# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
# TECHNICAL, ADVANCED CONFIGURATION
# 
# ---------------------------------------
# CHUNKING
# The commented options below give the default chunking options.
# Objects per chunk are selected from the target chunk size (16 MB) and 
# adjusted based on min/max objects per chunk, and the max bytes per chunk.
# It is important that the chunkCache (created on a per dataset basis) be 
# large enough to hold at least one chunk, ideally all chunks we need to have
# open at one time when writing to the dataset (usually one, unless we repair
# split events):
 
# chunkSizeTargetInBytes = 1703936 (16MB)
# chunkSizeTargetObjects = 0 (0 means select objects per chunk from chunkSizeInBytes)
# maxChunkSizeInBytes = 10649600  (100MB)
# minObjectsPerChunk = 50              
# maxObjectsPerChunk = 2048
# chunkCacheSizeTargetInChunks = 3
# maxChunkCacheSizeInBytes = 10649600  (100MB)

# ---------------------------------------
# REFINED DATASET CONTROL
#
# There are six classes of datasets for which individual options for shuffle,
# deflate, chunkSizeTargetInBytes and chunkSizeTargetObjects can be specified:
#
# regular (most everything, all psana types)
# epics (all the epics pv's)
# damage (accompanies all regular data from event store)
# ndarrays (new data from other modules)
# string's (new data from other modules)
# eventId (the time dataset that also accompanies all regular data, epics pvs, ndarrays and strings)
#
# The options for regular datasets have been discussed above. The other five datasets 
# get their default values for shuffle, deflate, chunkSizeInBytes and chunkSizeInObjects
# from the regular dataset options except in the cases below:
 
# damageShuffle = false
# stringShuffle = false
# epicsPvShuffle = false
# stringDeflate = -1
# eventIdChunkSizeTargetInBytes = 16384
# epicsPvChunkSizeTargetInBytes = 16384

# The rest of the shuffle, deflate and chunk size options for the other five datasets are:
#
# eventIdShuffle = true
# eventIdDeflate = 1
# damageDeflate = 1
# epicsPvDeflate = 1
# ndarrayShuffle = true
# ndarrayDeflate = 1
# eventIdChunkSizeTargetObjects = 0
# damageChunkSizeTargetInBytes = 1703936
# damageChunkSizeTargetObjects = 0
# stringChunkSizeTargetInBytes = 1703936
# stringChunkSizeTargetObjects = 0
# ndarrayChunkSizeTargetInBytes = 1703936
# ndarrayChunkSizeTargetObjects = 0
# epicsPvChunkSizeTargetObjects = 0

# ---------------------------------------
# SPLIT EVENTS
# When the Translator encounters a split event, it checks a cache to see
# if it has already seen it.  If it has, it fills in any blanks that it can.
# To prevent this cache from growing to large, set the maximum number of
# split events to look back through here (default is 3000):

max_saved_split_events = 3000

::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
::::template:::: type_aliases_cpp
::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
:
:  Template for a psana.cfg file that uses the Hdf5 Translator module.
:  This sets default values for all of the type filters.
:  
:  Parameters for this template:
:
:  base_headers - list of the include files
:  type_aliases - list of objects, each has a alias and typeList - the latter is a list of types
:  
/* Do not edit this file.  It is created by a code generator. */

#include "Translator/TypeAliases.h"

{% for baseheader in base_headers %}
#include "psddl_psana/{{baseheader}}"
{% endfor %}
#include "ndarray/ndarray.h"

using namespace Translator;
using namespace std;

TypeAliases::TypeAliases() {
{% for entry in type_aliases %}
  TypeInfoSet {{entry.alias}};
{% for type in entry.typeList %}
  {{entry.alias}}.insert( & typeid({{type}}));
{% endfor %}
  m_alias2TypesMap["{{entry.alias}}"] = {{entry.alias}};

{% endfor %}

  Alias2TypesMap::iterator pos;
  for (pos = m_alias2TypesMap.begin(); pos != m_alias2TypesMap.end(); ++pos) {
    m_aliasKeys.insert(pos->first);
    TypeInfoSet & typeSet = pos->second;
    TypeInfoSet::iterator typePos;
    for (typePos = typeSet.begin(); typePos != typeSet.end(); ++typePos) {
      m_type2AliasMap[*typePos] = pos->first;
    }
  }  
}

::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
::::template:::: hdfwritermap_cpp
::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
:
:  Template for a Translator/src/HdfWriterMap.cpp.  One function in this file, 
:  initializeHdfWriterMap is generated as it uses all the Psana types.
:  
:  Parameters for this template:
:  base_headers  - list of the basename of the header files in the psddl_hdf2psana package 
:             that include the store, store_at, make_datasets functions that we will use.  
:
:  namespaces - list of the namespaces that organize the store, store_at, make_datasets functions
:               in the psddl_hdf2psana package.
:  
:  psana_types - list of the psana types that we generate converters for
:
/* Do not edit this file.  It is created from a code generator.
Edit the template which resides in 

psddl/data/templates/hdf5Translator.tmpl?hdfwritermap_cpp
*/

#include "PSEvt/EventKey.h"
#include "PSEvt/TypeInfoUtils.h"
#include "Translator/HdfWriterFromEvent.h"
#include "Translator/HdfWriterMap.h"
#include "MsgLogger/MsgLogger.h"
{% for header in base_headers %}
#include "psddl_hdf2psana/{{header}}"
{% endfor %}
#include "Translator/HdfWriterNDArray.h"
#include "Translator/HdfWriterStringFromEvent.h"
 
using namespace std;

namespace {

const char *logger = "Translator.HdfWriterMap";
const int latestTypeSchema = -1;

using namespace Translator;
using namespace psddl_hdf2psana;
{% for namespace in namespaces %}
using namespace psddl_hdf2psana::{{namespace}};
{% endfor %}


template<typename T>
class HdfWriterPsana : public HdfWriterFromEvent {
public:
  void make_datasets(DataTypeLoc dataTypeLoc, hdf5pp::Group & srcGroup, 
                     const PSEvt::EventKey & eventKey, 
                     PSEvt::Event & evt, PSEnv::Env & env,
                     bool shuffle, int deflate,
                     boost::shared_ptr<Translator::ChunkPolicy> chunkPolicy)
  {
    boost::shared_ptr<T> ptr = getFromEventStore<T>(eventKey, dataTypeLoc, evt, env);
    MsgLog(logger, trace, "HdfWriter<" << PSEvt::TypeInfoUtils::typeInfoRealName(& typeid(T) ) << ">::make_datasets in group: " << srcGroup.name());
    ::make_datasets(*ptr,srcGroup,*chunkPolicy,deflate,shuffle,latestTypeSchema);
  }

  void store(DataTypeLoc dataTypeLoc, 
             hdf5pp::Group & srcGroup, 
             const PSEvt::EventKey & eventKey, 
             PSEvt::Event & evt, 
             PSEnv::Env & env) 
  {
    boost::shared_ptr<T> ptr = getFromEventStore<T>(eventKey, dataTypeLoc, evt, env);
    ::store(*ptr,srcGroup,latestTypeSchema); 
  }
  
  void store_at(DataTypeLoc dataTypeLoc, 
                long index, hdf5pp::Group & srcGroup, 
                const PSEvt::EventKey & eventKey, 
                PSEvt::Event & evt, 
                PSEnv::Env & env) {
    boost::shared_ptr<T> ptr = getFromEventStore<T>(eventKey, dataTypeLoc, evt, env);
    ::store_at(ptr.get(),srcGroup, index, latestTypeSchema); 
  }

  void append(DataTypeLoc dataTypeLoc,
              hdf5pp::Group & srcGroup, const PSEvt::EventKey & eventKey, 
              PSEvt::Event & evt, PSEnv::Env & env) 
  {
    boost::shared_ptr<T> ptr = getFromEventStore<T>(eventKey, dataTypeLoc, evt, env);
    ::store_at(ptr.get(), srcGroup, indexForAppend, latestTypeSchema);
  }

  void addBlank(hdf5pp::Group & group)
  {
    T *ptrForBlank = NULL;
    ::store_at(ptrForBlank,group, indexForAppend, latestTypeSchema);
  }
  static const long indexForAppend = -1;
};  // class HdfWriterPsana<T>


} // local namespace 

namespace Translator {


void HdfWriterMap::initialize() {
{% for psana_type in psana_types %}
  m_mainMap[ & typeid({{psana_type}}) ] = boost::make_shared<HdfWriterPsana<{{psana_type}}> >();
{% endfor %}

  // ndarrays
{% for elem, ndim in elemDimPairs %}
  m_mainMap[ & typeid(ndarray< {{elem}}, {{ndim}}>) ] = boost::make_shared<HdfWriterNDArray< {{elem}}, {{ndim}}, false > >();  
{% endfor %}
  // string               
  m_mainMap[ & typeid(std::string) ] = boost::make_shared<HdfWriterStringFromEvent>();

  // vlen ndarrays
{% for elem, ndim in elemDimPairs %}
  m_vlenMap[ & typeid(ndarray< {{elem}}, {{ndim}}>) ] = boost::make_shared<HdfWriterNDArray< {{elem}}, {{ndim}}, true > >();  
{% endfor %}
}

bool HdfWriterMap::remove(const std::type_info * typeInfoPtr)
{
  MapImpl::iterator posMain = m_mainMap.find(typeInfoPtr);
  MapImpl::iterator posVlen = m_vlenMap.find(typeInfoPtr);
  bool inMain = posMain != m_mainMap.end();
  bool inVlen = posVlen != m_vlenMap.end();
  if ((not inMain) and (not inVlen)) return false;
  if (inMain) m_mainMap.erase(posMain);
  if (inVlen) m_vlenMap.erase(posVlen);
  return true;
}

bool HdfWriterMap::replace(const std::type_info * typeInfoPtr,
             boost::shared_ptr<HdfWriterFromEvent> hdfWriter, bool vlen )
{
  MapImpl *map = &m_mainMap;
  if (vlen) map = &m_vlenMap;
  bool replaced = (map->find(typeInfoPtr) != map->end());
  (*map)[typeInfoPtr] = hdfWriter;
  return replaced;
}

boost::shared_ptr<HdfWriterFromEvent> HdfWriterMap::find(const std::type_info * typeInfoPtr, bool vlen)
{
  boost::shared_ptr<HdfWriterFromEvent> writer;
  MapImpl *map = &m_mainMap;
  if (vlen) map = &m_vlenMap;
  MapImpl::iterator pos = map->find(typeInfoPtr);
  if (pos == map->end()) return writer;
  writer = pos->second;
  return writer;
}

std::vector<const std::type_info *>  HdfWriterMap::types(bool vlenTable) {
  MapImpl *mapPtr = &m_mainMap;
  if (vlenTable) mapPtr = &m_vlenMap;
  vector<const type_info *> types(mapPtr->size());
  unsigned idx = 0;
  for (MapImpl::iterator pos = mapPtr->begin(); pos != mapPtr->end(); ++pos) {
    types.at(idx++) = pos->first;
  }
  return types;
}


} // Translator namespace


::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
::::template:::: epics_ddl_h
::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
:
:  Template for a Translator/include/epics.dll.h
:
:  Parameters for this template:
:  base_headers  - list of the basename of the header files in the psddl_hdf2psana package 
:             that include the store, store_at, make_datasets functions that we will use.  
:
:  namespaces - list of the namespaces that organize the store, store_at, make_datasets functions
:               in the psddl_hdf2psana package.
:  
:  psana_types - list of the psana types that we generate converters for
:
#ifndef TRANSLATOR_EPICS_DDL_H
#define TRANSLATOR_EPICS_DDL_H

/* ****************************
** Do not edit this file.  It is auto generated. **

The code is generated from psddl/src/DdlHdf5Translator.py 
  and the template in      psddl/data/templates/hdf5Translator.tmpl?XXX

The DDL defines the 14 epics classes via a C++ class hierarchy. 
These 14 types are EpicsPvCtrl* and EpicsPvTime* where * is one of:
String, Short, Float, Enum, Char, Long or Double.

An example of the hierarchy is

EpicsPvTimeDouble  derives from 
EpicsPvTimeHeader  derives from
EpicsPvHeader

moreover, classes have attributes that are themselves classes - the
dbr and stamp attributes.

We use the DDL description of the Epics Pv to 'unroll' the 14 epicsPv
types into  'flat' structures.  All fields within dbr are brought into
the top level. stamp in not flattened, so as not to deviate from the previous hdf5 schema.


For each of the 14 classes, we define a flat structure (except for stamp) in the Unroll
namespace. These structs hold one value.  See EpicsWriteBuffer for the templatized class
that will hold all the values.

Note for Unroll::EpicsPvCtrlEnum - space is allocated for the maximum number of 
enum string constants and the first value is stored after this. The hdf5 type will
only store the number of string constants used, with the values occuring after these.

 */

#include <string.h>

#include "hdf5/hdf5.h"
#include "psddl_psana/epics.ddl.h"

namespace Translator {

namespace Unroll {

struct epicsTimeStamp {
  uint32_t secPastEpoch;
  uint32_t nsec;
  epicsTimeStamp & operator=(const Psana::Epics::epicsTimeStamp & rhs) {
    secPastEpoch = rhs.sec();
    nsec = rhs.nsec();
    return *this;
 };
};

{% for epicsPv in epicsPvs %}
struct {{epicsPv['name']}} {
  typedef Psana::Epics::{{epicsPv['name']}} PsanaSrc;
  typedef {{epicsPv['value_basetype']}} valueBaseType{{epicsPv['value_array_print_info']}};
{% for attr in epicsPv['attrs'] %}
  {{attr['basetype']}} {{attr['name']}}{{attr['array_print_info']}};
{% endfor %} 
  {{epicsPv['value_basetype']}} value{{epicsPv['value_array_print_info']}};
};

{% endfor %}
} // namespace Unroll

{% for epicsPv in epicsPvs %}
void copyToUnrolledExceptForValue(const Psana::Epics::{{epicsPv['name']}} &source, Unroll::{{epicsPv['name']}} &dest);
{% endfor %} 

} // namespace Translator

#endif

::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
::::template:::: epics_ddl_cpp
::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
:
:  Template for Translator/src/epics.dll.cpp
:
:  Parameters for this template:
:  base_headers  - list of the basename of the header files in the psddl_hdf2psana package 
:             that include the store, store_at, make_datasets functions that we will use.  
:
:  namespaces - list of the namespaces that organize the store, store_at, make_datasets functions
:               in the psddl_hdf2psana package.
:  
:  psana_types - list of the psana types that we generate converters for
:
/* Do not edit this file.  It is created by a code generator. */

#include <string.h>
#include <algorithm>
#include "MsgLogger/MsgLogger.h"
#include "Translator/epics.ddl.h"

namespace {

void copyEpicsPvCtrlEnumStrings(const Psana::Epics::EpicsPvCtrlEnum & sourceObject, 
                                Translator::Unroll::EpicsPvCtrlEnum & destObject)
{
  const Psana::Epics::dbr_ctrl_enum& dbr = sourceObject.dbr();
  for (uint16_t stringNumber = 0; stringNumber < dbr.no_str(); ++stringNumber) {
    strncpy(destObject.strs[stringNumber], dbr.strings(stringNumber), Psana::Epics::MAX_ENUM_STRING_SIZE);
  }
}

} // local namespace

namespace Translator {

{% for epicsPv in epicsPvs %}
void copyToUnrolledExceptForValue(const Psana::Epics::{{epicsPv['name']}} &source,
                    Unroll::{{epicsPv['name']}} &dest) 
{
{% for attr in epicsPv['attrs'] %}
{% if attr['assignment'] == 'normal' %}
  dest.{{attr['name']}} = source.{{attr['accessor']}};
{% endif %}
{% if attr['assignment'] == 'enumstr' %}
  copyEpicsPvCtrlEnumStrings(source, dest);
{% endif %}
{% if attr['assignment'] == 'strncpy' %}
  strncpy(dest.{{attr['name']}}, source.{{attr['accessor']}}, Psana::Epics::{{attr['strncpy_max']}});
{% endif %}
{% endfor %}
}

{% endfor %} 

 
} // Translator

::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
::::template:::: dispatch_cpp
::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
:
:  Template for Translator/src/HdfWriterEpicsPvDispatch.cpp
:
:  Parameters for this template:
:  base_headers  - list of the basename of the header files in the psddl_hdf2psana package 
:             that include the store, store_at, make_datasets functions that we will use.  
:
:  namespaces - list of the namespaces that organize the store, store_at, make_datasets functions
:               in the psddl_hdf2psana package.
:  
:  psana_types - list of the psana types that we generate converters for
:
/* Do not edit this file. It is generated by a code generator.
*/
#include "MsgLogger/MsgLogger.h"
#include "psddl_psana/epics.ddl.h"
#include "Translator/epics.ddl.h"
#include "Translator/HdfWriterEpicsPv.h"

using namespace Translator;

namespace {
  const char * logger = "HdfWriterEpicsPv";
}

void HdfWriterEpicsPv::dispatch(hid_t groupId, int16_t dbrType, 
                                PSEnv::EpicsStore & epicsStore, 
                                const std::string & epicsPvName,
                                boost::shared_ptr<PSEvt::EventId> eventId,
                                DispatchAction dispatchAction) {
  switch (dbrType) {
{% for el in dbrTypes %}
  case Psana::Epics::{{el['dbr_str']}}:
    doDispatchAction<Unroll::{{el['pv_type']}}>(dbrType, "{{el['dbr_str']}}", 
                             "Psana::Epics::{{el['pv_type']}}",
                             groupId, epicsStore, epicsPvName, 
                             eventId, dispatchAction);
    break;
{% endfor %}
  default:
    MsgLog(logger, warning, "unexpected dbr type: " << dbrType << " in dispatch");
  }
}

