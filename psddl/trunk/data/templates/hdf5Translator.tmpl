:
:  Library of Jinja2 templates for Hdf5Translator backend
:
:  Lines starting with colon are comments, except for special '::::template::::'
:
::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
::::template:::: macros
::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
::::template:::: psana_cfg_template
::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
:
:  Template for a psana.cfg file that uses the Hdf5 Translator module.
:  This sets default values for all of the type filters.
:  
:  Parameters for this template:
:
: type_filter_options - a multiline list of the aliases, includes, and a comment
:                       to list all the types associated with the alias
######################################################################
[psana]

# MODULES: any modules that produce data to be translated need be loaded 
# **BEFORE** Translator.H5Output (such as calibrated data or NDArray's)
# event data added by modules listed after Translator.H5Output is not translated.
modules = Translator.H5Output

files = **TODO: SPECIFY INPUT FILES OR DATA SOURCE HERE**

######################################################################
[Translator.H5Output]

# TODO: enter the full h5 output file name, including the output directory
output_file = output_directory/h5output.h5

# # # # # # # # # # # # # # # # # # # # #
# EPICS FILTERING
# The Translator defaults to store all Epics Pv's:

store_epics = true

# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
# TYPE FILTERING 
#
# One can include or exclude a class of Psana types with the following 
# options. Only the strings include or exclude are valid for these 
# type filtering options. 
# 
# Note - Epics in the list below refers only to the epicsConfig data
# which is the alias list, not the epics pv's (see store_epics above for those)

{{type_filter_options}}


# # # # # # # # # # # # # # # # # # # # # # # # # # # # #
# SOURCE FILTERING
#
# The default for the src_filter option is "include all"
# If you want to include a subset of the sources, do
#
# src_filter include srcname1 srcname2  
#
#  or if you want to exclude a subset of sources, do
#
# src_filter exclude srcname1 srcname2
#
# The srcname's in xtc input can be printed using the EventKeys
# module.  For example
#
# psana -n 5 -m EventKeys exp=cxi1235:run=33 
#
# Will print the EventKeys in the first 5 events.  If the output includes
#
#   EventKey(type=Psana::EvrData::DataV3, src=DetInfo(NoDetector.0:Evr.2))
#   EventKey(type=Psana::CsPad::DataV2, src=DetInfo(CxiDs1.0:Cspad.0))
#   EventKey(type=Psana::CsPad2x2::ElementV1, src=DetInfo(CxiSc2.0:Cspad2x2.1))
#   EventKey(type=Psana::Bld::BldDataEBeamV3, src=BldInfo(EBeam))
#   EventKey(type=Psana::Bld::BldDataFEEGasDetEnergy, src=BldInfo(FEEGasDetEnergy))
#   EventKey(type=Psana::Camera::FrameV1, src=BldInfo(CxiDg2_Pim))
#
# Then one can filter on these six srcname's:
#
#  NoDetector.0:Evr.2  CxiDs1.0:Cspad.0  CxiSc2.0:Cspad2x2.1  EBeam  FEEGasDetEnergy  CxiDg2_Pim
#

src_filter = include all

# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
# CALIBRATION FILTERING
#
# Psana calibration modules can produce a calibrated version of CsPad data
# (The data types CsPad::DataV1 or CsPad::DataV2). The module output will be 
# data of the same type and src as the uncalibrated data, with an additional key, 
# such as 'calibrated'.
#
# The Translator defaults to skipping the translation of the uncalibrated
# data when a calibrated version of that data is present.  Below you 
# can control the calibration key and whether or not to include the
# uncalibrated data.

calibration_key = calibrated
include_uncalibrated_data = false

# Note: this only affects calibrated data of the same type and src as the
# uncalibrated data.  When the calibration module produces a NDArray, both
# the NDArray and the uncalibrated data are translated.  If you do not wish
# to translate the uncalibrated data, use appropriate type or src_filter options.
# Likewise if you do not want to translate certain NDArray's, see the 
# ndarray_key_filter options below.

# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
# NDARRAY AND STD::STRING KEY FILTERING
#
# A number of NDArray's and any std::string found in the event store are translated into 
# the hdf5 file.  NDarray's up to 4 dimensions of 10 basic types (8, 16, 32 and 64 bit 
# signed and unsigned int, float and double) are translated, but see the comment after the 
# ndarray_types option in the type filtering section for the most up to date list.
#
# These NDArray's and std::string's can be filtered by specifying the eventKey key that was
# used to put the data in the event.  While a srcname and key uniquely distinguish data in the
# event store, the Translator filter's NDArray's and std::string's using only the
# key string. The default is to include all ndarray's and std::string's found:

ndarray_key_filter = include all
std_string_key_filter = include all

# an example of including only one ndarray (with keystring being 'finalanswer') would be
#
# ndarray_key_filter include finalanswer
#
# and several ndarrays or strings can be included or excluded
#
# ndarray_key_filter = exclude arrayA arrayB
# std_string_key_filter = include message1 eventDescription

# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # 
# COMPRESSION 
#
# The following options control compression for most all datasets.
# Shuffling improves compression for certain datasets. Valid values for
# deflate (gzip compression level) are 0-9. Setting deflate = -1 turns off
# compression.

shuffle = True
deflate = 1

# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
# TECHNICAL, ADVANCED CONFIGURATION
# 
# ---------------------------------------
# CHUNKING
# The commented options below give the default chunking options.
# Objects per chunk are selected from the target chunk size (16 MB) and 
# adjusted based on min/max objects per chunk, and the max bytes per chunk.
# It is important that the chunkCache (created on a per dataset basis) be 
# large enough to hold at least one chunk, ideally all chunks we need to have
# open at one time when writing to the dataset (usually one, unless we repair
# split events):
 
# chunkSizeTargetInBytes = 1703936 (16MB)
# chunkSizeTargetObjects = 0 (0 means select objects per chunk from chunkSizeInBytes)
# maxChunkSizeInBytes = 10649600  (100MB)
# minObjectsPerChunk = 50              
# maxObjectsPerChunk = 2048
# chunkCacheSizeTargetInChunks = 3
# maxChunkCacheSizeInBytes = 10649600  (100MB)

# ---------------------------------------
# REFINED DATASET CONTROL
#
# There are six classes of datasets for which individual options for shuffle,
# deflate, chunkSizeTargetInBytes and chunkSizeTargetObjects can be specified:
#
# regular (most everything, all psana types)
# epics (all the epics pv's)
# damage (accompanies all regular data from event store)
# ndarrays (new data from other modules)
# string's (new data from other modules)
# eventId (the time dataset that also accompanies all regular data, epics pvs, ndarrays and strings)
#
# The options for regular datasets have been discussed above. The other five datasets 
# get their default values for shuffle, deflate, chunkSizeInBytes and chunkSizeInObjects
# from the regular dataset options except in the cases below:
 
# damageShuffle = false
# stringShuffle = false
# epicsPvShuffle = false
# stringDeflate = -1
# eventIdChunkSizeTargetInBytes = 16384
# epicsPvChunkSizeTargetInBytes = 16384

# The rest of the shuffle, deflate and chunk size options for the other five datasets are:
#
# eventIdShuffle = true
# eventIdDeflate = 1
# damageDeflate = 1
# epicsPvDeflate = 1
# ndarrayShuffle = true
# ndarrayDeflate = 1
# eventIdChunkSizeTargetObjects = 0
# damageChunkSizeTargetInBytes = 1703936
# damageChunkSizeTargetObjects = 0
# stringChunkSizeTargetInBytes = 1703936
# stringChunkSizeTargetObjects = 0
# ndarrayChunkSizeTargetInBytes = 1703936
# ndarrayChunkSizeTargetObjects = 0
# epicsPvChunkSizeTargetObjects = 0

# ---------------------------------------
# SPLIT EVENTS
# When the Translator encounters a split event, it checks a cache to see
# if it has already seen it.  If it has, it fills in any blanks that it can.
# To prevent this cache from growing to large, set the maximum number of
# split events to look back through here (default is 3000):

max_saved_split_events = 3000

# ---------------------------------------
# HDF5 GROUP NAMES
# The typenames for beam line data defaults to being written as (for example) 
# Bld::BldDataEBeamV0. Setting short_bld_name to true causes it to be 
# written as BldDataEBeamV0. If set to true, names are written differently
# then with o2o-translate and the change may break code that reads h5 files 
# (such as psana)

short_bld_name = false

# ---------------------------------------
# HDF5 FILE PROPERTIES
#
# split large files, presently we only support NoSplit. Future options may be: Family and SplitScan
# for future splitting, splitSize defaults to 10 GB
split = NoSplit    
splitSize = 10737418240      

::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
::::template:::: type_aliases_cpp
::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
:
:  Template for a psana.cfg file that uses the Hdf5 Translator module.
:  This sets default values for all of the type filters.
:  
:  Parameters for this template:
:
:  base_headers - list of the include files
:  type_aliases - list of objects, each has a alias and typeList - the latter is a list of types
:  
/* Do not edit this file.  It is created by a code generator. */

#include "Translator/TypeAliases.h"

{% for baseheader in base_headers %}
#include "psddl_psana/{{baseheader}}"
{% endfor %}
#include "ndarray/ndarray.h"

using namespace Translator;
using namespace std;

TypeAliases::TypeAliases() {
{% for entry in type_aliases %}
  TypeInfoSet {{entry.alias}};
{% for type in entry.typeList %}
  {{entry.alias}}.insert( & typeid({{type}}));
{% endfor %}
  m_alias2TypesMap["{{entry.alias}}"] = {{entry.alias}};

{% endfor %}

  Alias2TypesMap::iterator pos;
  for (pos = m_alias2TypesMap.begin(); pos != m_alias2TypesMap.end(); ++pos) {
    m_aliasKeys.insert(pos->first);
    TypeInfoSet & typeSet = pos->second;
    TypeInfoSet::iterator typePos;
    for (typePos = typeSet.begin(); typePos != typeSet.end(); ++typePos) {
      m_type2AliasMap[*typePos] = pos->first;
    }
  }  
}

::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
::::template:::: hdfwritermap_cpp
::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
:
:  Template for a Translator/src/HdfWriterMap.cpp.  One function in this file, 
:  initializeHdfWriterMap is generated as it uses all the Psana types.
:  
:  Parameters for this template:
:  base_headers  - list of the basename of the header files in the psddl_hdf2psana package 
:             that include the store, store_at, make_datasets functions that we will use.  
:
:  namespaces - list of the namespaces that organize the store, store_at, make_datasets functions
:               in the psddl_hdf2psana package.
:  
:  psana_types - list of the psana types that we generate converters for
:
/* Do not edit this file.  It is created from a code generator.
Edit the template which resides in 

psddl/data/templates/hdf5Translator.tmpl?hdfwritermap_cpp
*/

#include "PSEvt/EventKey.h"
#include "PSEvt/TypeInfoUtils.h"
#include "Translator/HdfWriterFromEvent.h"
#include "Translator/HdfWriterMap.h"
#include "MsgLogger/MsgLogger.h"
{% for header in base_headers %}
#include "psddl_hdf2psana/{{header}}"
{% endfor %}
#include "Translator/HdfWriterNDArray.h"
#include "Translator/HdfWriterStringFromEvent.h"
 
using namespace std;

namespace {

const char *logger = "Translator.HdfWriterMap";
const int latestTypeSchema = -1;

using namespace Translator;
using namespace psddl_hdf2psana;
{% for namespace in namespaces %}
using namespace psddl_hdf2psana::{{namespace}};
{% endfor %}


template<typename T>
class HdfWriterPsana : public HdfWriterFromEvent {
public:
  void make_datasets(DataTypeLoc dataTypeLoc, hdf5pp::Group & srcGroup, 
                     const PSEvt::EventKey & eventKey, 
                     PSEvt::Event & evt, PSEnv::Env & env,
                     bool shuffle, int deflate,
                     boost::shared_ptr<Translator::ChunkPolicy> chunkPolicy)
  {
    boost::shared_ptr<T> ptr = getFromEventStore<T>(eventKey, dataTypeLoc, evt, env);
    MsgLog(logger, trace, "HdfWriter<" << PSEvt::TypeInfoUtils::typeInfoRealName(& typeid(T) ) << ">::make_datasets in group: " << srcGroup.name());
    ::make_datasets(*ptr,srcGroup,*chunkPolicy,deflate,shuffle,latestTypeSchema);
  }

  void store(DataTypeLoc dataTypeLoc, 
             hdf5pp::Group & srcGroup, 
             const PSEvt::EventKey & eventKey, 
             PSEvt::Event & evt, 
             PSEnv::Env & env) 
  {
    boost::shared_ptr<T> ptr = getFromEventStore<T>(eventKey, dataTypeLoc, evt, env);
    ::store(*ptr,srcGroup,latestTypeSchema); 
  }
  
  void store_at(DataTypeLoc dataTypeLoc, 
                long index, hdf5pp::Group & srcGroup, 
                const PSEvt::EventKey & eventKey, 
                PSEvt::Event & evt, 
                PSEnv::Env & env) {
    boost::shared_ptr<T> ptr = getFromEventStore<T>(eventKey, dataTypeLoc, evt, env);
    ::store_at(ptr.get(),srcGroup, index, latestTypeSchema); 
  }

  void append(DataTypeLoc dataTypeLoc,
              hdf5pp::Group & srcGroup, const PSEvt::EventKey & eventKey, 
              PSEvt::Event & evt, PSEnv::Env & env) 
  {
    boost::shared_ptr<T> ptr = getFromEventStore<T>(eventKey, dataTypeLoc, evt, env);
    ::store_at(ptr.get(), srcGroup, indexForAppend, latestTypeSchema);
  }

  void addBlank(hdf5pp::Group & group)
  {
    T *ptrForBlank = NULL;
    ::store_at(ptrForBlank,group, indexForAppend, latestTypeSchema);
  }
  static const long indexForAppend = -1;
};  // class HdfWriterPsana<T>


} // local namespace 

namespace Translator {

boost::shared_ptr<HdfWriterFromEvent> getHdfWriter(HdfWriterMap & mapping, const std::type_info *typeInfoPtr) {
  //  static map<HdfWriterFromEvent*,set<std::type_info*> > convertersNotFound;
   HdfWriterMap::iterator pos = mapping.find(typeInfoPtr);
  if (pos == mapping.end()) {
    // TODO: move this log message into H5Output
    //    WithMsgLog(logger,trace,str) {
    //      // to avoid printing messages for every event about converters not found, we do some simple
    //      // caching here. The first time we see that a converter is not found for a type, we log a 
    //      //  message. After that, we do not.  Since this function may be called for different 
    //      set<std::type_info*> notFoundThisMap = convertersNotFound[&mapping];
    //      if (notFoundThisMap.find(typeInfoPtr) == notFoundThisMap.end()) {
    //        str << "getHdfWriter: no converter found for type " << PSEvt::typeInfoRealName(typeInfoPtr);
    //        convertersNotFound.insert(typeInfoPtr);
    //      }
    //    }
    return boost::shared_ptr<Translator::HdfWriterFromEvent>();
  }
  return pos->second;
}

void initializeHdfWriterMap( HdfWriterMap & mapping) {
{% for psana_type in psana_types %}
  mapping[ & typeid({{psana_type}}) ] = boost::make_shared<HdfWriterPsana<{{psana_type}}> >();
{% endfor %}

  // ndarrays
{% for elem, ndim in elemDimPairs %}
  mapping[ & typeid(ndarray< {{elem}}, {{ndim}}>) ] = boost::make_shared<HdfWriterNDArray< {{elem}}, {{ndim}} > >();  
{% endfor %}
  // string               
  mapping[ & typeid(std::string) ] = boost::make_shared<HdfWriterStringFromEvent>();
}

} // Translator namespace


::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
::::template:::: epics_ddl_h
::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
:
:  Template for a Translator/include/epics.dll.h
:
:  Parameters for this template:
:  base_headers  - list of the basename of the header files in the psddl_hdf2psana package 
:             that include the store, store_at, make_datasets functions that we will use.  
:
:  namespaces - list of the namespaces that organize the store, store_at, make_datasets functions
:               in the psddl_hdf2psana package.
:  
:  psana_types - list of the psana types that we generate converters for
:
#ifndef TRANSLATOR_EPICS_DDL_H
#define TRANSLATOR_EPICS_DDL_H

/* ****************************
** Do not edit this file.  It is auto generated. **

The code is generated from psddl/src/DdlHdf5Translator.py 
  and the template in      psddl/data/templates/hdf5Translator.tmpl?XXX

We use the DDL description of the Epics Pv to 'unroll' the 14 epicsPv
types that we will write into simple 'flat' structures that make it easier
to generate the hdf5 types.  Flat except for stamp, so as not to deviate from
the previous schema, we do not unroll the stamp field into seconds past the epoch
and nanoseconds. These 14 types are EpicsPvCtrl* and EpicsPvTime*
where * is one of String, Short, Float, Enum, Char, Long or Double.

The DDL defines these classes via a C++ class hierarchy, and there are some
compound objects that make up some of the attributes.  For example 

EpicsPvTimeDouble  -> EpicsPvTimeHeader -> EpicsPvHeader

moreover, classes can have compound types, for instance EpicsPvTimeHeader includes an
instance of epicsTimeStamp as an attribute.

For each of the 14 classes, we define a flat structure (except for stamp) that we can fill for Hdf5
translation, a function to produce the hdf5 type, and a function to fill the structure from
the appropriate Psana object that we will obtain from the epics store.

A difference between entries stored in the hdf5 datasets and the epics pv's from xtc, is the following.
EpicsPvTimeLong has an attribute NumElements() which says how many longs are stored (back to back, 
in an array) in the data() field.  If there are 3 longs in a particular epics pv, lets say the
pv name is "LASERTIMING", then for one psana epics pv of "LASERTIMING", we will store 3 entries 
in the "LASERTIMING" dataset of the Hdf5 file.  Each entry has one long in it.

 */

#include <string.h>

#include "hdf5/hdf5.h"
#include "psddl_psana/epics.ddl.h"

namespace Translator {

namespace Unroll {

struct epicsTimeStamp {
  uint32_t secPastEpoch;
  uint32_t nsec;
  epicsTimeStamp & operator=(const Psana::Epics::epicsTimeStamp & rhs) {
    secPastEpoch = rhs.sec();
    nsec = rhs.nsec();
    return *this;
 };
};

{% for epicsPv in epicsPvs %}
struct {{epicsPv['name']}} {
  typedef Psana::Epics::{{epicsPv['name']}} PsanaSrc;
{% for attr in epicsPv['attrs'] %}
  {{attr['basetype']}} {{attr['name']}}{{attr['array_print_info']}};
{% endfor %} 
  {{epicsPv['value_basetype']}} value{{epicsPv['value_array_print_info']}};
};

{% endfor %}
} // namespace Unroll

{% for epicsPv in epicsPvs %}
void copyToUnrolled(const Psana::Epics::{{epicsPv['name']}} &source, const int16_t element, Unroll::{{epicsPv['name']}} &dest);
{% endfor %} 

template <class U>
void copyValueFldToUnrolled(const typename U::PsanaSrc &psanaVar, int16_t el, U & unrollBuffer) {
  unrollBuffer.value = psanaVar.value(el);
}

template <>
void copyValueFldToUnrolled < Unroll::EpicsPvTimeString >
       (const Unroll::EpicsPvTimeString::PsanaSrc &psanaVar, int16_t el, 
        Unroll::EpicsPvTimeString & unrollBuffer);

template <>
void copyValueFldToUnrolled < Unroll::EpicsPvCtrlString >
       (const Unroll::EpicsPvCtrlString::PsanaSrc &psanaVar, int16_t el, 
        Unroll::EpicsPvCtrlString & unrollBuffer);

template <>
void copyValueFldToUnrolled < Unroll::EpicsPvCtrlEnum >
       (const Unroll::EpicsPvCtrlEnum::PsanaSrc &psanaVar, int16_t el, 
        Unroll::EpicsPvCtrlEnum & unrollBuffer);

hid_t createH5TypeId_epicsTimeStamp();

{% for epicsPv in epicsPvs %}
hid_t createH5TypeId_{{epicsPv['name']}}({{epicsPv['type_create_args']}});
{% endfor %} 

template <class U>
int getNumberStringsForCtrlEnum(boost::shared_ptr<typename U::PsanaSrc> ptr) { return -1; }
 
template <>
int getNumberStringsForCtrlEnum<Unroll::EpicsPvCtrlEnum>(boost::shared_ptr<Unroll::EpicsPvCtrlEnum::PsanaSrc> ptr);

} // namespace Translator

#endif

::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
::::template:::: epics_ddl_cpp
::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
:
:  Template for Translator/src/epics.dll.cpp
:
:  Parameters for this template:
:  base_headers  - list of the basename of the header files in the psddl_hdf2psana package 
:             that include the store, store_at, make_datasets functions that we will use.  
:
:  namespaces - list of the namespaces that organize the store, store_at, make_datasets functions
:               in the psddl_hdf2psana package.
:  
:  psana_types - list of the psana types that we generate converters for
:
/* Do not edit this file.  It is created by a code generator. */

#include <string.h>
#include "MsgLogger/MsgLogger.h"
#include "Translator/epics.ddl.h"

namespace {

const char * logger = "epics.dll";

void copyEpicsPvCtrlEnumStrings(const Psana::Epics::EpicsPvCtrlEnum & sourceObject, 
                                Translator::Unroll::EpicsPvCtrlEnum & destObject)
{
  const Psana::Epics::dbr_ctrl_enum& dbr = sourceObject.dbr();
  for (uint16_t stringNumber = 0; stringNumber < dbr.no_str(); ++stringNumber) {
    strncpy(destObject.strs[stringNumber], dbr.strings(stringNumber), Psana::Epics::MAX_ENUM_STRING_SIZE);
  }
}

} // local namespace

namespace Translator {

{% for epicsPv in epicsPvs %}
void copyToUnrolled(const Psana::Epics::{{epicsPv['name']}} &source, const int16_t element, 
                    Unroll::{{epicsPv['name']}} &dest) 
{
{% for attr in epicsPv['attrs'] %}
{% if attr['assignment'] == 'normal' %}
  dest.{{attr['name']}} = source.{{attr['accessor']}};
{% endif %}
{% if attr['assignment'] == 'enumstr' %}
  copyEpicsPvCtrlEnumStrings(source, dest);
{% endif %}
{% if attr['assignment'] == 'strncpy' %}
  strncpy(dest.{{attr['name']}}, source.{{attr['accessor']}}, Psana::Epics::{{attr['strncpy_max']}});
{% endif %}
{% endfor %}
  copyValueFldToUnrolled<Unroll::{{epicsPv['name']}}>(source,element,dest);
}

{% endfor %} 

template <>
void copyValueFldToUnrolled < Unroll::EpicsPvTimeString >
       (const Unroll::EpicsPvTimeString::PsanaSrc &psanaVar, int16_t el, 
        Unroll::EpicsPvTimeString & unrollBuffer) {
  strncpy(unrollBuffer.value, psanaVar.value(el), Psana::Epics::MAX_STRING_SIZE);
}

template <>
void copyValueFldToUnrolled < Unroll::EpicsPvCtrlString >
       (const Unroll::EpicsPvCtrlString::PsanaSrc &psanaVar, int16_t el, 
        Unroll::EpicsPvCtrlString & unrollBuffer) {
  strncpy(unrollBuffer.value, psanaVar.value(el), Psana::Epics::MAX_STRING_SIZE);
}

template <>
void copyValueFldToUnrolled < Unroll::EpicsPvCtrlEnum >
       (const Unroll::EpicsPvCtrlEnum::PsanaSrc &psanaVar, int16_t el, 
        Unroll::EpicsPvCtrlEnum & unrollBuffer) {
  int numberOfStrs = psanaVar.dbr().no_str();
  void * valueAddress = &unrollBuffer.strs[numberOfStrs];
  uint16_t * valuePtr = static_cast<uint16_t*>(valueAddress);
  MsgLog(logger,info,"copyValueFldToUnRolled EpicsPvCtrlEnum el=" << el);
  *valuePtr = psanaVar.value(el);  
}

hid_t createH5TypeId_epicsTimeStamp() {
  hid_t typeId = H5Tcreate(H5T_COMPOUND, sizeof(Unroll::epicsTimeStamp));
  if (typeId < 0) MsgLog(logger, fatal, "Failed to create h5 type id for epicsTimeStamp");
  herr_t status = 0;
  status = std::min(status, H5Tinsert(typeId, "secPastEpoch", offsetof(Unroll::epicsTimeStamp, secPastEpoch), H5T_NATIVE_INT32));
  status = std::min(status, H5Tinsert(typeId, "nsec", offsetof(Unroll::epicsTimeStamp, nsec), H5T_NATIVE_INT32));
  if (status < 0) MsgLog(logger, fatal, "error inserting field into h5 typeId for epicsTimeStamp"); 
  return typeId;
}

 
{% for epicsPv in epicsPvs if epicsPv['name'] != 'EpicsPvCtrlEnum' %}
hid_t createH5TypeId_{{epicsPv['name']}}({{epicsPv['type_create_args']}}) {
  hid_t typeId = H5Tcreate(H5T_COMPOUND, sizeof(Unroll::{{epicsPv['name']}}));
  if (typeId < 0) MsgLog(logger, fatal, "Failed to create h5 type id for {{epicsPv['name']}}");
  herr_t status = 0;
{% for attr in epicsPv['attrs'] %}
  status = std::min(status, H5Tinsert(typeId, "{{attr['h5name']}}", offsetof(Unroll::{{epicsPv['name']}}, {{attr['name']}}), {{attr['h5type']}}));
{% endfor %} 
  status = std::min(status, H5Tinsert(typeId, "value", offsetof(Unroll::{{epicsPv['name']}}, value), {{epicsPv['value_h5type']}}));

  if (status < 0) MsgLog(logger, fatal, "error inserting field into h5 typeId for {{epicsPv['name']}}"); 

  return typeId;
}

{% endfor %} 

hid_t createH5TypeId_EpicsPvCtrlEnum(hid_t pvNameType, hid_t strsArrayType, int numberOfStrings) {
  hid_t typeId = H5Tcreate(H5T_COMPOUND, sizeof(Unroll::EpicsPvCtrlEnum));
  if (typeId < 0) MsgLog(logger, fatal, "Failed to create h5 type id for EpicsPvCtrlEnum");
  herr_t status = 0;
  status = std::min(status, H5Tinsert(typeId, "pvId", offsetof(Unroll::EpicsPvCtrlEnum, iPvId), H5T_NATIVE_INT16));
  status = std::min(status, H5Tinsert(typeId, "dbrType", offsetof(Unroll::EpicsPvCtrlEnum, iDbrType), H5T_NATIVE_INT16));
  status = std::min(status, H5Tinsert(typeId, "numElements", offsetof(Unroll::EpicsPvCtrlEnum, iNumElements), H5T_NATIVE_INT16));
  status = std::min(status, H5Tinsert(typeId, "pvname", offsetof(Unroll::EpicsPvCtrlEnum, sPvName), pvNameType));
  status = std::min(status, H5Tinsert(typeId, "status", offsetof(Unroll::EpicsPvCtrlEnum, status), H5T_NATIVE_INT16));
  status = std::min(status, H5Tinsert(typeId, "severity", offsetof(Unroll::EpicsPvCtrlEnum, severity), H5T_NATIVE_INT16));
  status = std::min(status, H5Tinsert(typeId, "no_str", offsetof(Unroll::EpicsPvCtrlEnum, no_str), H5T_NATIVE_INT16));
  status = std::min(status, H5Tinsert(typeId, "strs", offsetof(Unroll::EpicsPvCtrlEnum, strs), strsArrayType));
  size_t offsetForValue = offsetof(Unroll::EpicsPvCtrlEnum, strs) +  numberOfStrings * Psana::Epics::MAX_ENUM_STRING_SIZE;
  status = std::min(status, H5Tinsert(typeId, "value", offsetForValue, H5T_NATIVE_UINT16));

  if (status < 0) MsgLog(logger, fatal, "error inserting field into h5 typeId for EpicsPvCtrlEnum"); 

  return typeId;
}
  
template <>
int getNumberStringsForCtrlEnum<Unroll::EpicsPvCtrlEnum>(boost::shared_ptr<Unroll::EpicsPvCtrlEnum::PsanaSrc> ptr) 
{ 
  return ptr->dbr().no_str(); 
}
 
} // Translator

::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
::::template:::: dispatch_cpp
::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
:
:  Template for Translator/src/HdfWriterEpicsPvDispatch.cpp
:
:  Parameters for this template:
:  base_headers  - list of the basename of the header files in the psddl_hdf2psana package 
:             that include the store, store_at, make_datasets functions that we will use.  
:
:  namespaces - list of the namespaces that organize the store, store_at, make_datasets functions
:               in the psddl_hdf2psana package.
:  
:  psana_types - list of the psana types that we generate converters for
:

#include "MsgLogger/MsgLogger.h"
#include "psddl_psana/epics.ddl.h"
#include "Translator/epics.ddl.h"
#include "Translator/HdfWriterEpicsPv.h"

using namespace Translator;

namespace {
  const char * logger = "HdfWriterEpicsPv";
}

void HdfWriterEpicsPv::dispatch(hid_t groupId, int16_t dbrType, 
                                PSEnv::EpicsStore & epicsStore, 
                                const std::string & epicsPvName,
                                boost::shared_ptr<PSEvt::EventId> eventId,
                                DispatchAction dispatchAction) {
  switch (dbrType) {
{% for el in dbrTypes %}
  case Psana::Epics::{{el['dbr_str']}}:
    doDispatchAction<Unroll::{{el['pv_type']}}>(dbrType, "{{el['dbr_str']}}", 
                             "Psana::Epics::{{el['pv_type']}}",
                             groupId, epicsStore, epicsPvName, 
                             eventId, dispatchAction);
    break;
{% endfor %}
  default:
    MsgLog(logger, warning, "unexpected dbr type: " << dbrType << " in dispatch");
  }
}

